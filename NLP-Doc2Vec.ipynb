{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucaskenjis\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Imports para NLP\n",
    "#from nltk import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  negative\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remover review duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.review.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remover colunas null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~df.review.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpeza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método para limpeza de texto aplicando:\n",
    "    - replace para um caractere específico\n",
    "    - isdigit para verificar se a palavra é um digito e assim, remove-la.\n",
    "    - limpeza de tag html usando BeautifulSoup\n",
    "    - regexp_tokenize para transformar textos em tokens, e coletar apenas palavras através do regex [\\w]+. Ignorando acentos.\n",
    "    - .lower() para deixar as palavras minusculas\n",
    "    - stopwords para remover palavras que não agregam informações\n",
    "    - stemming para extrair o radical das palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "ps = PorterStemmer()\n",
    "eng_stpw = set(stopwords.words('english'))\n",
    "\n",
    "def padronizardados(text):\n",
    "    # remover caractere especifico\n",
    "    text = text.replace('\\ ','')\n",
    "    # remover digitos\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    # remover tags html\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    #tokenization e lower case\n",
    "    text = regexp_tokenize(soup.get_text().lower(),\"[\\w]+\")\n",
    "    #remover stopwords\n",
    "    temp = []\n",
    "    for t in text:\n",
    "        if t not in eng_stpw:\n",
    "            #aplicar o stemming\n",
    "            temp.append(ps.stem(t))\n",
    "    new_text = (' '.join(temp))\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Texto antes do método"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Taut and organically gripping, Edward Dmytryk\\'s Crossfire is a distinctive suspense thriller, an unlikely \"message\" movie using the look and devices of the noir cycle.<br /><br />Bivouacked in Washington, DC, a company of soldiers cope with their restlessness by hanging out in bars. Three of them end up at a stranger\\'s apartment where Robert Ryan, drunk and belligerent, beats their host (Sam Levene) to death because he happens to be Jewish. Police detective Robert Young investigates with the help of Robert Mitchum, who\\'s assigned to Ryan\\'s outfit. Suspicion falls on the second of the three (George Cooper), who has vanished. Ryan slays the third buddy (Steve Brodie) to insure his silence before Young closes in.<br /><br />Abetted by a superior script by John Paxton, Dmytryk draws precise performances from his three starring Bobs. Ryan, naturally, does his prototypical Angry White Male (and to the hilt), while Mitchum underplays with his characteristic alert nonchalance (his role, however, is not central); Young may never have been better. Gloria Grahame gives her first fully-fledged rendition of the smart-mouthed, vulnerable tramp, and, as a sad sack who\\'s leeched into her life, Paul Kelly haunts us in a small, peripheral role that he makes memorable.<br /><br />The politically engaged Dmytryk perhaps inevitably succumbs to sermonizing, but it\\'s pretty much confined to Young\\'s reminiscence of how his Irish grandfather died at the hands of bigots a century earlier (thus, incidentally, stretching chronology to the limit). At least there\\'s no attempt to render an explanation, however glib, of why Ryan hates Jews (and hillbillies and...).<br /><br />Curiously, Crossfire survives even the major change wrought upon it -- the novel it\\'s based on (Richard Brooks\\' The Brick Foxhole) dealt with a gay-bashing murder. But homosexuality in 1947 was still Beyond The Pale. News of the Holocaust had, however, begun to emerge from the ashes of Europe, so Hollywood felt emboldened to register its protest against anti-Semitism (the studios always quaked at the prospect of offending any potential ticket buyer).<br /><br />But while the change from homophobia to anti-Semitism works in general, the specifics don\\'t fit so smoothly. The victim\\'s chatting up a lonesome, drunk young soldier then inviting him back home looks odd, even though (or especially since) there\\'s a girlfriend in tow. It raises the question whether this scenario was retained inadvertently or left in as a discreet tip-off to the original engine generating Ryan\\'s murderous rage.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aplicando método de limpeza aos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review = df.review.apply(padronizardados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Texto depois do método"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'taut organ grip edward dmytryk crossfir distinct suspens thriller unlik messag movi use look devic noir cycl bivouack washington dc compani soldier cope restless hang bar three end stranger apart robert ryan drunk belliger beat host sam leven death happen jewish polic detect robert young investig help robert mitchum assign ryan outfit suspicion fall second three georg cooper vanish ryan slay third buddi steve brodi insur silenc young close abet superior script john paxton dmytryk draw precis perform three star bob ryan natur prototyp angri white male hilt mitchum underplay characterist alert nonchal role howev central young may never better gloria graham give first fulli fledg rendit smart mouth vulner tramp sad sack leech life paul kelli haunt us small peripher role make memor polit engag dmytryk perhap inevit succumb sermon pretti much confin young reminisc irish grandfath die hand bigot centuri earlier thu incident stretch chronolog limit least attempt render explan howev glib ryan hate jew hillbilli curious crossfir surviv even major chang wrought upon novel base richard brook brick foxhol dealt gay bash murder homosexu still beyond pale news holocaust howev begun emerg ash europ hollywood felt embolden regist protest anti semit studio alway quak prospect offend potenti ticket buyer chang homophobia anti semit work gener specif fit smoothli victim chat lonesom drunk young soldier invit back home look odd even though especi sinc girlfriend tow rais question whether scenario retain inadvert left discreet tip origin engin gener ryan murder rage'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformar sentiments em 0 e 1 (Label encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "tag = le.fit_transform(df.sentiment)\n",
    "tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec\n",
    "    A técnica Doc2Vec é baseada na Word2Vec. A técninca consiste na representação da palavra através de um vetor de números e essa é a grande diferença para as técnicas CountVectorizer e TFIDF. Com essa representação, podemos plotar os vetores em um plano,além de aplicarmos cálculos para verificar a relação entre as palavras, como sinônimos, antônimos, analogias e etc...\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](word2vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A técnica conta com a utilização de dois algoritmos: \n",
    "        - Continuous Bag of words: Usado para prever palavra target a partir de palavras de um contexto.\n",
    "        EX: _______ que li e concordo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](cbow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Skip-Gram: Usado para prever as palavras de um contexto a partir de uma palavra target.\n",
    "        EX: Declaro ___ __ _ ________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](skip-gram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " O Doc2Vec nada mais é do que a técninca de Word2Vec aplicada para Documentos, independente do tamanho. Para isso, foi adicionado um novo paramentro ao Bag of words (ID). Por isso a necessidade da utilização do TaggedDocuments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split dados\n",
    "    - Optei por separar os dados em treino, validação e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "\n",
    "test, val = train_test_split(test, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformando os dados no formato Texto, tag com TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_tagged = [TaggedDocument(doc, [i]) for i, doc in enumerate(X_train)]\n",
    "#val_tagged = [TaggedDocument(doc, [i]) for i, doc in enumerate(X_val)]\n",
    "#test_tagged = [TaggedDocument(doc, [i]) for i, doc in enumerate(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=r['review'].split(' '), tags=[r.sentiment]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=r['review'].split(' '), tags=[r.sentiment]), axis=1)\n",
    "val_tagged = val.apply(\n",
    "    lambda r: TaggedDocument(words=r['review'].split(' '), tags=[r.sentiment]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['one', 'review', 'mention', 'watch', 'oz', 'episod', 'hook', 'right', 'exactli', 'happen', 'first', 'thing', 'struck', 'oz', 'brutal', 'unflinch', 'scene', 'violenc', 'set', 'right', 'word', 'go', 'trust', 'show', 'faint', 'heart', 'timid', 'show', 'pull', 'punch', 'regard', 'drug', 'sex', 'violenc', 'hardcor', 'classic', 'use', 'word', 'call', 'oz', 'nicknam', 'given', 'oswald', 'maximum', 'secur', 'state', 'penitentari', 'focus', 'mainli', 'emerald', 'citi', 'experiment', 'section', 'prison', 'cell', 'glass', 'front', 'face', 'inward', 'privaci', 'high', 'agenda', 'em', 'citi', 'home', 'mani', 'aryan', 'muslim', 'gangsta', 'latino', 'christian', 'italian', 'irish', 'scuffl', 'death', 'stare', 'dodgi', 'deal', 'shadi', 'agreement', 'never', 'far', 'away', 'would', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'goe', 'show', 'dare', 'forget', 'pretti', 'pictur', 'paint', 'mainstream', 'audienc', 'forget', 'charm', 'forget', 'romanc', 'oz', 'mess', 'around', 'first', 'episod', 'ever', 'saw', 'struck', 'nasti', 'surreal', 'say', 'readi', 'watch', 'develop', 'tast', 'oz', 'got', 'accustom', 'high', 'level', 'graphic', 'violenc', 'violenc', 'injustic', 'crook', 'guard', 'sold', 'nickel', 'inmat', 'kill', 'order', 'get', 'away', 'well', 'manner', 'middl', 'class', 'inmat', 'turn', 'prison', 'bitch', 'due', 'lack', 'street', 'skill', 'prison', 'experi', 'watch', 'oz', 'may', 'becom', 'comfort', 'uncomfort', 'view', 'that', 'get', 'touch', 'darker', 'side'], tags=['positive'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando o modelo Doc2Vec com o vocabulario de train_tagged\n",
    "    - min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
    "    - window (int, optional) – The maximum distance between the current and predicted word within a sentence.\n",
    "    - vector_size (int, optional) – Dimensionality of the feature vectors.\n",
    "    - workers (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "model_d2v = Doc2Vec(train_tagged,min_count=3,\n",
    "                     window=5,\n",
    "                     vector_size=100,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_d2v.build_vocab([x for x in (train_tagged.values)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinando o modelo\n",
    "The parameters:\n",
    "\n",
    "- dados de treino ( text,tag)\n",
    "- total_examples = int - Count of sentences;\n",
    "- epochs = int - Number of iterations (epochs) over the corpus - [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 21.97 mins\n"
     ]
    }
   ],
   "source": [
    "model_d2v.train(train_tagged, total_examples=model_d2v.corpus_count,epochs=30)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time.time() - start_time) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39665,), (4958,), (4959,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.shape,test_tagged.shape,val_tagged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação do modelo para embedding\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    model.random.seed(1)\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformação e criação dos dados de treino, validação e teste. Assim como suas variáveis target\n",
    "y_train, X_train = vec_for_learning(model_d2v, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_d2v, test_tagged)\n",
    "y_val,X_val = vec_for_learning(model_d2v, val_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalização\n",
    "from sklearn import preprocessing\n",
    "\n",
    "normalized_train = preprocessing.normalize(X_train)\n",
    "normalized_test = preprocessing.normalize(X_test)\n",
    "normalized_val = preprocessing.normalize(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39665, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection + L1 e Regressao logistica\n",
    "    - Seleção de features utilizando um loop para testar alguns pesos para o paramentro C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.35 minutos \n",
      "\n",
      "27.4 minutos \n",
      "\n",
      "27.44 minutos \n",
      "\n",
      "27.49 minutos \n",
      "\n",
      "27.54 minutos \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cs=[.01,.1,1,10,100]\n",
    "#\n",
    "summary=[]\n",
    "\n",
    "\n",
    "for c in cs:\n",
    "    \n",
    "    #seleção de atributos\n",
    "    logreg = LogisticRegression(solver='saga', penalty='l1',C=c, max_iter=500).fit(normalized_train, y_train)\n",
    "    select_features = SelectFromModel(logreg, prefit=True)\n",
    "    \n",
    "    X_train_sel=select_features.transform(normalized_train)\n",
    "    X_test_sel=select_features.transform(normalized_test)\n",
    "    X_val_sel=select_features.transform(normalized_val)\n",
    "\n",
    "    #fittando o modelo\n",
    "    model_d2v = LogisticRegression(solver='saga', max_iter=100).fit(X_train_sel, y_train)\n",
    "    \n",
    "    #avaliando acurácia\n",
    "    model_d2v_score = model_d2v.score(X_val_sel, y_val)\n",
    "    \n",
    "    #resumo da validação\n",
    "    summary.append((c,np.shape(X_train_sel)[1],model_d2v_score))\n",
    "    \n",
    "    print(round((time.time() - start_time)/60,2),\"minutos \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01 Features=25 Acc=0.7512\n",
      "C=0.10 Features=81 Acc=0.8080\n",
      "C=1.00 Features=98 Acc=0.8106\n",
      "C=10.00 Features=100 Acc=0.8104\n",
      "C=100.00 Features=100 Acc=0.8104\n"
     ]
    }
   ],
   "source": [
    "for i in summary:\n",
    "  print(\"C=%.2f Features=%d Acc=%3.4f\" %i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação do modelo com o melhor peso C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.12 minutos \n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='saga', penalty='l1',C=1, max_iter=100).fit(normalized_train, y_train)\n",
    "select_features = SelectFromModel(logreg, prefit=True)\n",
    "    \n",
    "X_train_sel=select_features.transform(normalized_train)\n",
    "X_test_sel=select_features.transform(normalized_test)\n",
    "X_val_sel=select_features.transform(normalized_val)\n",
    "\n",
    "model = LogisticRegression(solver='saga', max_iter=100).fit(X_train_sel, y_train)\n",
    "    \n",
    "logregScore = model.score(X_val_sel, y_val)\n",
    "    \n",
    "summary.append((c,np.shape(X_train_sel)[1],logregScore))\n",
    "    \n",
    "print(round((time.time() - start_time)/60,2),\"minutos \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando o modelo criado com as variáveis teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia na base de teste=0.811 \n",
      "\n",
      "[[2058  499]\n",
      " [ 438 1963]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Acurácia na base de teste=%3.3f \\n\" % model.score(X_test_sel, y_test))\n",
    "\n",
    "y_pred = model.predict(X_test_sel)\n",
    "print(confusion_matrix(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
